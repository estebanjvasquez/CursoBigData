{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4O9rJfBooD2t+vuN/A5b4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Modelos no Supervisados\n","\n","------------------------------------------------------\n","\n","\n","### Data Science and Machine Learning\n","\n","#### Febrero 2023\n","\n","**Aurora Cobo Aguilera**\n","\n","**The Valley**\n","\n","------------------------------------------------------"],"metadata":{"id":"-ZcyaHqmJwq1"}},{"cell_type":"markdown","source":["# Contenidos que se van a tratar a lo largo de la sesión\n","---\n","\n","- Parte I: [Reducción de dimensionalidad y extracción de características con un modelo lineal: PCA](#PCA)\n","- Parte II: [Clustering o agrupación de datos con K-MEANS](#kmeans)\n","- Parte III: [Estimación de densidades de probabilidad con modelos de mezcla Gaussianos](#GMM)"],"metadata":{"id":"V9FzsgU-M_Zk"}},{"cell_type":"code","metadata":{"id":"6eIN4_rcP7wx"},"source":["from IPython.core.display import Image, display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ka1IoY0aP7xB"},"source":["%matplotlib inline\n","%config InlineBackend.figure_format = 'retina' "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdtByAKnP7xM"},"source":["import matplotlib.pyplot as plt\n","from matplotlib import rc\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Configuración de las figuras matplotlib\n","plt.rcParams['figure.figsize'] = [8, 6]\n","plt.rcParams.update({'font.size': 8})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLt2RZ59P7xT"},"source":["def draw_vector(v0, v1, ax=None):\n","    ax = ax or plt.gca()\n","    arrowprops=dict(arrowstyle='->',\n","                    linewidth=2,\n","                    shrinkA=0, shrinkB=0,color='r')\n","    ax.annotate('', v1, v0, arrowprops=arrowprops)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0arPp7mhP7xb"},"source":["from scipy.stats import multivariate_normal, norm\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.linear_model import LogisticRegression as LR\n","\n","from warnings import filterwarnings\n","filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaZTbzOMP7xl"},"source":["def load_spam():\n","    data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',header=None)\n","    data.columns=[\"wf_make\",         \n","        \"wf_address\",      \n","        \"wf_all\",          \n","        \"wf_3d\",           \n","        \"wf_our\",          \n","        \"wf_over\",         \n","        \"wf_remove\",       \n","        \"wf_internet\",     \n","        \"wf_order\",        \n","        \"wf_mail\",         \n","        \"wf_receive\",      \n","        \"wf_will\",         \n","        \"wf_people\",       \n","        \"wf_report\",       \n","        \"wf_addresses\",    \n","        \"wf_free\",         \n","        \"wf_business\",     \n","        \"wf_email\",        \n","        \"wf_you\",          \n","        \"wf_credit\",       \n","        \"wf_your\",         \n","        \"wf_font\",         \n","        \"wf_000\",          \n","        \"wf_money\",        \n","        \"wf_hp\",           \n","        \"wf_hpl\",          \n","        \"wf_george\",       \n","        \"wf_650\",          \n","        \"wf_lab\",          \n","        \"wf_labs\",         \n","        \"wf_telnet\",       \n","        \"wf_857\",          \n","        \"wf_data\",         \n","        \"wf_415\",          \n","        \"wf_85\",           \n","        \"wf_technology\",   \n","        \"wf_1999\",         \n","        \"wf_parts\",        \n","        \"wf_pm\",           \n","        \"wf_direct\",       \n","        \"wf_cs\",           \n","        \"wf_meeting\",      \n","        \"wf_original\",     \n","        \"wf_project\",      \n","        \"wf_re\",           \n","        \"wf_edu\",          \n","        \"wf_table\",        \n","        \"wf_conference\",   \n","        \"cf_;\",            \n","        \"cf_(\",            \n","        \"cf_[\",            \n","        \"cf_!\",            \n","        \"cf_$\",            \n","        \"cf_#\",            \n","        \"cap_average\", \n","        \"cap_longest\", \n","        \"cap_total\",\n","        \"target\"]\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xadDmOlrP7x8"},"source":["def load_wine():\n","    data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header = None)\n","    \n","    data.columns = [\"Wine_type\", \"Alcohol\", \"Malic_acid\", \"Ash\", \"Alcalinity\", \"Magnesium\", \"Total_phenols\",\"Flavanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\",\n","                 \"Color_intensity\", \"Hue\", \"OD280-OD315_diluted\",\"Proline\"]\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2jFQ6fsP7yL"},"source":["# Aprendizaje  no supervisado\n","---\n","\n","- No se dispone de un (*target*) para cada observación\n","- Detectar o descubrir **patrones** tales como grupos en las observaciones.\n","- Se hace una predicción por cada *target*\n","- Ejemplos típicos: *K means*, *PCA*, *GMMs*, *spectral clustering*, *CCA*\n","- Aplicaciones:\n"," - agrupar colecciones de datos\n"," - limpieza de *outliers*\n"," - segmentar vídeo o audio\n"," - segmentación de clientes\n"," - organización de colecciones de documentos\n"," - aprendizaje de funciones de densidad de probabilidad\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2f7-djqQP7yM"},"source":["<a id='PCA'></a>\n","\n","# Parte I: Análisis de Componentes Principales (PCA)\n","---\n","\n","PCA es un método de aprendizaje no supervisado muy eficiente para reducir la dimensionalidad de conjunto de datos. Supongamos que tenemos $N$ datos de dimensión $D$, $\\mathbf{x}_{n}\\in\\mathbb{R}^D$ para $n=1,\\ldots,N$. Supongamos que son vectores columna de dimensión $D\\times 1$, esto es $\\mathbf{x}^T_{n}=[x_{n1}, x_{n2}, \\ldots, x_{nD}]$. Además, por simplicidad suponemos que los datos tienen media cero\n","\n","$$\\overline{\\mathbf{x}} = \\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_{n}=\\mathbf{0}$$\n","\n","En PCA, vamos a realizar una reducción de dimensionalidad lineal. Esto es, para cada dato $\\bf x$, calculamos una representación $\\mathbf{z}\\in\\mathbb{R}^K$ en un espacio de dimensión $K$, donde $K<D$, mediante una multiplicación matricial \n","\n","$$ {\\bf z} = [z_{1}, z_{2}, \\ldots, z_{K}] = {\\bf x}^T {\\bf U}$$\n","\n","donde la matriz de transformación $\\bf U$ tiene dimensiones $D\\times K$ \n","\n","$${\\bf U}=[\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots,\\mathbf{u}_{K}]$$\n","\n","y $\\mathbf{u}_{j}$, $j=1\\ldots K$, son las columnas de $\\bf U$. Además, estas columnas son ortonormales\n","\n","$$\\mathbf{u}_{j}^T\\mathbf{u}_{j} = 1$$\n","\n","$$\\mathbf{u}_{j}^T\\mathbf{u}_{j} = 0 ~~i \\neq j$$ \n","\n","En estas condiciones, observad que $z_{j}= {\\bf x}^T \\mathbf{u}_j$ es la **proyección** de $\\bf x$ sobre $\\mathbf{u}_j$. \n","\n","\n","<img src='http://www.tsc.uc3m.es/~olmos/BBVA/proyeccionv3.jpeg' width=600 />\n","\n","Finalmente, puesto que $\\bf U$ está formada por columnas ortonormales y por tanto $\\bf UU^T=I$, dado el vector $\\bf z$ podemos reconstruir el vector en el espacio de dimensión $D$ de la forma\n","\n","$$\\tilde{\\mathbf{x}} = \\mathbf{U} {\\bf z}^T  $$\n","\n","En general $\\tilde{\\mathbf{x}}\\neq \\mathbf{x}$, puesto que al reducir dimensionalidad siempre perdemos cierta información. Caracterizaremos este error midiendo el error cuadrático medio\n","\n","$$J = \\frac{1}{N}\\sum_{n=1}^{D} ||\\mathbf{x}_n - \\tilde{\\mathbf{x}}_n||^2_2 =\\frac{1}{N}\\sum_{n=1}^{D} ||\\mathbf{x}_n - \\mathbf{U}\\mathbf{z}_n^T||^2_2$$"]},{"cell_type":"markdown","metadata":{"id":"cSqY4gUFP7yO"},"source":["## Un ejemplo sintético en dos dimensiones\n","\n","Supongamos el siguiente conjunto de datos"]},{"cell_type":"code","metadata":{"id":"6Bpu4ELPP7yQ","scrolled":true},"source":["# Fijamos la semilla\n","rng = np.random.RandomState(1)\n","A = rng.rand(2, 2)\n","\n","X = (A@rng.randn(2, 100)).T\n","\n","print(\"Hemos generado {0:d} datos de {1:d} dimensiones\".format(X.shape[0],X.shape[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PyLFGaVP7ya"},"source":["plt.scatter(X[:, 0], X[:, 1])\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.axis('equal')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dO7lx4IuP7yi"},"source":["Ciertamente, los datos siguen una tendencia lineal que nos dice que no perderíamos mucha información si proyectáramos cada punto en una línea recta definida por un vector unitario $\\mathbf{u}_1$. A continuación vamos a estudiar el error de reconstrucción $J$ para distintos valores de $\\mathbf{u}_1$. En cada caso, vamos a representar también la **varianza de los datos proyectados sobre $\\mathbf{u}_1$**.\n","\n","Por ejemplo, para $\\mathbf{u}_1 = [\\cos(\\pi/4),\\sin(\\pi/4)]$ vamos a dibujar el histograma de la proyección de los datos sobre el vector y el histograma del error cuadrático de reconstrucción"]},{"cell_type":"code","metadata":{"id":"Qc8WgCx8P7yj"},"source":["a = np.pi/4\n","\n","u = np.array([np.cos(a),np.sin(a)]).reshape([-1,1]) # Pasamos a cartesianas\n","\n","\n","Z = X @ u # Proyecciones sobre u de los datos X\n","\n","X_r = Z @ u.T\n","\n","J = np.mean(np.linalg.norm(X-X_r,axis=1)**2)\n","\n","varianza = np.var(Z)\n","\n","print(\"El error de reconstrucción J es {0:.2f}\".format(J))\n","print(\"La varianza de los datos proyectados en el vector u es {0:.2f}\".format(varianza))\n","\n","nbins = 15\n","\n","fig, (ax_l, ax_c, ax_r) = plt.subplots(nrows=1, ncols=3, figsize=(16, 8))\n","ax_l.set_ylim([-1,1])\n","ax_l.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","ax_l.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","draw_vector(np.mean(X,axis=0).reshape(2,1), np.mean(X,axis=0).reshape(2,1) + u,ax_l)\n","ax_l.set_ylim(-2,2)\n","ax_l.set_title(\"Datos y vector u_1\")\n","\n","ax_c.hist(Z,nbins)\n","ax_c.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax_c.set_title(\"Histograma proyección sobre u_1\")\n","ax_c.text(0.75, 16, 'varianza={0:.2f}'.format(varianza), fontsize=13)\n","\n","\n","ax_r.hist(np.linalg.norm(X-X_r,axis=1)**2,nbins)\n","ax_r.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax_r.set_title(\"Histograma error cuadrático de reconstucción\")\n","ax_r.text(0.5, 51.5, 'Error medio J={0:.2f}'.format(J), fontsize=13)\n","plt.show();\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8sph6A4UP7yr"},"source":["Representemos ahora la varianza de las proyecciones y el error $J$ para $\\mathbf{u}_1 = [\\cos(a),\\sin(a)]$ a medida que variamos $a$ entre $[0,\\pi]$ (es decir entre 0 y 180 grados). Dibujamos también el vector $\\mathbf{u}$ para el cual el error de reconstrucción es menor"]},{"cell_type":"code","metadata":{"id":"liv0HW5IP7yr"},"source":["# Definimos todos los vectors u por si ángulo con respecto al eje x positivo\n","a_vector = np.arange(0,1,1./32)*np.pi\n","\n","varianza = []\n","\n","J = []\n","\n","for i,a in enumerate(a_vector):\n","    u = np.array([np.cos(a),np.sin(a)]).reshape([-1,1]) # Pasamos a cartesianas\n","    \n","    Z = X @ u # Proyecciones sobre u de los datos X\n","    \n","    X_r = Z @ u.T \n","    \n","    J.append(np.mean(np.linalg.norm(X-X_r,axis=1)**2))\n","\n","    varianza.append(np.var(Z))\n","    \n","a_opt = a_vector[np.argmin(J)]\n","u_opt = np.array([np.cos(a_opt),np.sin(a_opt)]).reshape([-1,1])\n","    \n","fig, (ax_l, ax_c, ax_r) = plt.subplots(nrows=1, ncols=3, figsize=(16, 8))\n","\n","ax_l.set_ylim([-1,1])\n","ax_l.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","ax_l.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","draw_vector(np.mean(X,axis=0).reshape(2,1), np.mean(X,axis=0).reshape(2,1) + u_opt,ax_l)\n","ax_l.set_ylim(-2,2)\n","ax_l.set_title(\"Datos y vector u optimo\")\n","\n","ax_c.plot(a_vector,varianza,'-s')\n","ax_c.plot(a_opt,np.max(varianza),'rs',ms=8)\n","ax_c.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax_c.set_xlabel(\"$a$\")\n","ax_c.set_ylabel(\"$J(a)$\")\n","ax_c.set_title(\"Varianza de las proyecciones\")\n","ax_c.text(1., 0.7, 'Varianza máxima J={0:.2f}'.format(np.min(J)), fontsize=13)\n","ax_c.text(1., 0.65, 'Mejor ángulo a={0:.2f} rads'.format(a_opt), fontsize=13)\n","\n","ax_r.plot(a_vector,J,'-s')\n","ax_r.plot(a_opt,np.min(J),'rs',ms=8)\n","ax_r.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax_r.set_xlabel(\"$a$\")\n","ax_r.set_ylabel(\"$J(a)$\")\n","ax_r.set_title(\"Error reconstrucción\")\n","ax_r.text(1., 0.1, 'Error mínimo J={0:.2f}'.format(np.min(J)), fontsize=13)\n","ax_r.text(1., 0.05, 'Mejor ángulo a={0:.2f} rads'.format(a_opt), fontsize=13)\n","    \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zYrTHMW_P7yy"},"source":["Podemos comprobar cómo el **mínimo error de reconstrucción se consigue para el vector que induce la mayor varianza en el espacio proyectado**. Esto no es casualidad. En PCA la selección de la matriz $\\bf U$ persigue maximizar la varianza proyectada, y se puede demostrar que esto minimiza el error de reconstrucción (como acabamos de comprobar experimentalmente)."]},{"cell_type":"markdown","metadata":{"id":"Kv29L733P7y0"},"source":["## Problema que resolvemos en PCA\n","---\n","\n","Con PCA, buscaremos la matriz $\\bf U$ tal que maximizamos la **varianza** de los datos en el espacio proyectado. Mas concretamente, la solución de PCA se calcula del siguiente modo\n","\n","- La componente principal 1, determinada por el vector $\\mathbf{u}_1$, es aquella que maximiza la varianza de los datos proyectados sobre $\\mathbf{u}_1$\n","\n","$$ \\max _{\\mathbf{u}_1} \\frac{1}{N} \\sum_{n=1}^{N} (z_{n,1})^2 = \\max _{\\mathbf{u}_1} \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}^T_{n}\\mathbf{u}_{1})^2$$\n","\n","\n","- La componente principal 2, determinada por el vector $\\mathbf{u}_2$, es aquella dirección perpendicular a la componente 1 que maximiza la varianza proyectada\n","\n","$$ \\max _{\\mathbf{u}_2} \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}^T_{n}\\mathbf{u}_{2})^2 \\text{ tal que } \\mathbf{u}_1^T\\mathbf{u}_2=0$$\n","\n","- La componente principal 3, determinada por el vector $\\mathbf{u}_3$, es aquella que es perpendicular a las componentes principales 1 y 2 y maximiza la varianza proyectada\n","\n","$$ \\max _{\\mathbf{u}_3} \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}^T_{n}\\mathbf{u}_{3})^2 \\text{ tal que } \\mathbf{u}_1^T\\mathbf{u}_3=0 \\text{ y } \\mathbf{u}_2^T\\mathbf{u}_3=0$$\n","\n","- Así sucesivamente hasta encontrar las $K$ componentes principales.\n"]},{"cell_type":"markdown","metadata":{"id":"yBFSeU4zP7y3"},"source":["## PCA sobre el ejemplo anterior\n","\n","\n","Vamos a utilizar la implementación de [PCA en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) para determinar las componentes principales. "]},{"cell_type":"code","metadata":{"id":"M87M5avNP7y5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668091173596,"user_tz":-60,"elapsed":1027,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"291121e6-06f2-48b6-d222-a95a050c036c"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","pca.fit(X)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PCA(n_components=2)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"2R-KBIv9P7y_"},"source":["Podemos acceder a los vectores $\\mathbf{u}_1$ y $\\mathbf{u}_2$ usando `pca.components_`, siendo una componente principal cada una de las filas de la matriz obtenida"]},{"cell_type":"code","metadata":{"id":"jlHnvTmGP7zC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668091205648,"user_tz":-60,"elapsed":304,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"22e14784-8289-4d7a-b66a-0551b6a3b4bc"},"source":["print(pca.components_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.95257252  0.30431168]\n"," [-0.30431168  0.95257252]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"yOj3x9A9P7zI"},"source":["Vamos a calcular la varianza de los datos proyectados en cada una de las componentes principales ..."]},{"cell_type":"code","metadata":{"id":"9GtoKDneP7zK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668091229786,"user_tz":-60,"elapsed":405,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"606f5638-b2e8-4d1d-9560-f3c70828f8f9"},"source":["n_components = 2\n","\n","proyecciones = pca.transform(X)  # Cálculo de las proyecciones\n","\n","for n in range(n_components):\n","    \n","    print(\"Varianza de las proyecciones sobre la componente {0:d} : {1:.2f}\".format(n,np.var(proyecciones[:,n])))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Varianza de las proyecciones sobre la componente 0 : 0.71\n","Varianza de las proyecciones sobre la componente 1 : 0.01\n"]}]},{"cell_type":"markdown","metadata":{"id":"fS9SbQ1sP7zP"},"source":["Estos mismos valores pueden obtenerse directamente del objeto PCA entrenado:"]},{"cell_type":"code","metadata":{"id":"Cuk-lznyP7zS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668091262934,"user_tz":-60,"elapsed":347,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"09a6f932-dd67-42ad-b414-e45594cee5f4"},"source":["n_components = 2\n","\n","for n in range(n_components):\n","    \n","    var = pca.explained_variance_[n]\n","    \n","    print(\"Varianza de las proyecciones sobre la componente {0:d} : {1:.2f}\".format(n,var))\n","    \n","  "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Varianza de las proyecciones sobre la componente 0 : 0.71\n","Varianza de las proyecciones sobre la componente 1 : 0.01\n"]}]},{"cell_type":"markdown","metadata":{"id":"anC9K7yEP7zX"},"source":["Finalmente, vamos a dibujar los vectores que definen las componentes principales sobre el dataset"]},{"cell_type":"code","metadata":{"id":"_BJ0rswcP7zZ"},"source":["# Dibujar gráfica de los datos\n","plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","for length, vector in zip(pca.explained_variance_, pca.components_):\n","    v = vector \n","    draw_vector(pca.mean_, pca.mean_ + v)\n","plt.axis('equal')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ug6Xs8tP7zi"},"source":["## Solución analítica de PCA\n","\n","- Se puede demostrar que las componentes principales $\\mathbf{u}_{1},\\mathbf{u}_{2},\\ldots,\\mathbf{u}_{K}$ se corresponden con los $K$ **autovectores** con mayor **autovalor** $(\\lambda_1,\\ldots,\\lambda_K)$ de la matriz de covarianza $C_{\\mathbf{X},\\mathbf{X}}$ de los datos. Si los datos tienen media nula, $C_{\\mathbf{X},\\mathbf{X}}$ se calcula como\n","\n","$$C_{\\mathbf{X},\\mathbf{X}} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_{n}\\mathbf{x}_{n}^T$$\n","\n","\n","- De hecho, la **varianza explicada en cada dimensión coincide con el autovalor correspondiente**. Por tanto, a mayor autovalor, más importancia tiene la componente correspondiente para explicar nuestros datos. La implementación de [PCA en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), nos ordena la solución de PCA en orden decreciente de autovalores, con lo que la **primera componente es siempre la más importante**.\n","\n","\n","- Finalmente, se puede demostrar también que la solución PCA **minimiza** el error de reconstucción cuadrático medio\n","\n","\n","$$J = \\frac{1}{N}\\sum_{n=1}^{D} ||\\mathbf{x}_n - \\tilde{\\mathbf{x}}_n||^2_2 =\\frac{1}{N}\\sum_{n=1}^{D} ||\\mathbf{x}_n - \\mathbf{U}\\mathbf{z}_n^T||^2_2$$\n"]},{"cell_type":"markdown","metadata":{"id":"DSG45gLDP7zj"},"source":["## Un ejemplo con datos reales\n","\n","Vamos a usar PCA para visualizar en 2 dimensiones un dataset ([cars93](https://www.tandfonline.com/doi/full/10.1080/10691898.1993.11910459)) que continene distintos atributos de 92 modelos distintos de coches, tales como cilindrada, consumo, precio, etc ..."]},{"cell_type":"code","metadata":{"id":"dFS_4kMJP7zk","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"ok","timestamp":1668091662093,"user_tz":-60,"elapsed":336,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"b24daa41-8eb5-42db-ca32-1634b023b5bd"},"source":["car_data = pd.read_csv('Cars93.csv')\n","\n","car_data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0 Manufacturer    Model     Type  Min.Price  Price  Max.Price  \\\n","0           1        Acura  Integra    Small       12.9   15.9       18.8   \n","1           2        Acura   Legend  Midsize       29.2   33.9       38.7   \n","2           3         Audi       90  Compact       25.9   29.1       32.3   \n","3           4         Audi      100  Midsize       30.8   37.7       44.6   \n","4           5          BMW     535i  Midsize       23.7   30.0       36.2   \n","\n","   MPG.city  MPG.highway             AirBags  ... Passengers Length  \\\n","0        25           31                None  ...          5    177   \n","1        18           25  Driver & Passenger  ...          5    195   \n","2        20           26         Driver only  ...          5    180   \n","3        19           26  Driver & Passenger  ...          6    193   \n","4        22           30         Driver only  ...          4    186   \n","\n","   Wheelbase  Width  Turn.circle  Rear.seat.room Luggage.room  Weight  \\\n","0        102     68           37            26.5         11.0    2705   \n","1        115     71           38            30.0         15.0    3560   \n","2        102     67           37            28.0         14.0    3375   \n","3        106     70           37            31.0         17.0    3405   \n","4        109     69           39            27.0         13.0    3640   \n","\n","    Origin           Make  \n","0  non-USA  Acura Integra  \n","1  non-USA   Acura Legend  \n","2  non-USA        Audi 90  \n","3  non-USA       Audi 100  \n","4  non-USA       BMW 535i  \n","\n","[5 rows x 28 columns]"],"text/html":["\n","  <div id=\"df-a1c11744-d21d-4054-a5a6-575472c8a90c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Manufacturer</th>\n","      <th>Model</th>\n","      <th>Type</th>\n","      <th>Min.Price</th>\n","      <th>Price</th>\n","      <th>Max.Price</th>\n","      <th>MPG.city</th>\n","      <th>MPG.highway</th>\n","      <th>AirBags</th>\n","      <th>...</th>\n","      <th>Passengers</th>\n","      <th>Length</th>\n","      <th>Wheelbase</th>\n","      <th>Width</th>\n","      <th>Turn.circle</th>\n","      <th>Rear.seat.room</th>\n","      <th>Luggage.room</th>\n","      <th>Weight</th>\n","      <th>Origin</th>\n","      <th>Make</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>Acura</td>\n","      <td>Integra</td>\n","      <td>Small</td>\n","      <td>12.9</td>\n","      <td>15.9</td>\n","      <td>18.8</td>\n","      <td>25</td>\n","      <td>31</td>\n","      <td>None</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>177</td>\n","      <td>102</td>\n","      <td>68</td>\n","      <td>37</td>\n","      <td>26.5</td>\n","      <td>11.0</td>\n","      <td>2705</td>\n","      <td>non-USA</td>\n","      <td>Acura Integra</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>Acura</td>\n","      <td>Legend</td>\n","      <td>Midsize</td>\n","      <td>29.2</td>\n","      <td>33.9</td>\n","      <td>38.7</td>\n","      <td>18</td>\n","      <td>25</td>\n","      <td>Driver &amp; Passenger</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>195</td>\n","      <td>115</td>\n","      <td>71</td>\n","      <td>38</td>\n","      <td>30.0</td>\n","      <td>15.0</td>\n","      <td>3560</td>\n","      <td>non-USA</td>\n","      <td>Acura Legend</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Audi</td>\n","      <td>90</td>\n","      <td>Compact</td>\n","      <td>25.9</td>\n","      <td>29.1</td>\n","      <td>32.3</td>\n","      <td>20</td>\n","      <td>26</td>\n","      <td>Driver only</td>\n","      <td>...</td>\n","      <td>5</td>\n","      <td>180</td>\n","      <td>102</td>\n","      <td>67</td>\n","      <td>37</td>\n","      <td>28.0</td>\n","      <td>14.0</td>\n","      <td>3375</td>\n","      <td>non-USA</td>\n","      <td>Audi 90</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>Audi</td>\n","      <td>100</td>\n","      <td>Midsize</td>\n","      <td>30.8</td>\n","      <td>37.7</td>\n","      <td>44.6</td>\n","      <td>19</td>\n","      <td>26</td>\n","      <td>Driver &amp; Passenger</td>\n","      <td>...</td>\n","      <td>6</td>\n","      <td>193</td>\n","      <td>106</td>\n","      <td>70</td>\n","      <td>37</td>\n","      <td>31.0</td>\n","      <td>17.0</td>\n","      <td>3405</td>\n","      <td>non-USA</td>\n","      <td>Audi 100</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>BMW</td>\n","      <td>535i</td>\n","      <td>Midsize</td>\n","      <td>23.7</td>\n","      <td>30.0</td>\n","      <td>36.2</td>\n","      <td>22</td>\n","      <td>30</td>\n","      <td>Driver only</td>\n","      <td>...</td>\n","      <td>4</td>\n","      <td>186</td>\n","      <td>109</td>\n","      <td>69</td>\n","      <td>39</td>\n","      <td>27.0</td>\n","      <td>13.0</td>\n","      <td>3640</td>\n","      <td>non-USA</td>\n","      <td>BMW 535i</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 28 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1c11744-d21d-4054-a5a6-575472c8a90c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a1c11744-d21d-4054-a5a6-575472c8a90c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a1c11744-d21d-4054-a5a6-575472c8a90c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"L64eXvAUP7zq"},"source":["# Nombre del coche juntando fabricante y modelo\n","car_data['Name'] = car_data['Manufacturer'] + car_data['Model']\n","\n","# Vamos a usar un total de 7 variables\n","columns_use = ['Price','MPG.highway','MPG.city','Horsepower','Fuel.tank.capacity','Passengers','Weight','Length']\n","\n","car_data_red = car_data[columns_use]\n","\n","car_data_red.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XHC9LUr1P7zv","scrolled":true,"executionInfo":{"elapsed":5,"status":"ok","timestamp":1668091723232,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"},"user_tz":-60},"outputId":"3d800998-0c6a-4435-f148-ab5a49296e2b"},"source":["list_cars=list(car_data['Name'])\n","\n","Data0 = np.array(car_data_red[car_data_red.columns])\n","\n","# Normalizamos los datos\n","\n","scaler = StandardScaler().fit(Data0)\n","\n","Data = scaler.transform(Data0)\n","\n","pca_cars = PCA(n_components=2)\n","\n","pca_cars.fit(Data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PCA(n_components=2)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"NdFtD0Q-P7z1"},"source":["# Proyectamos los datos en las dos componentes principales\n","\n","Data2d = pca_cars.transform(Data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZsANN8hP7z5"},"source":["from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","from skimage import io\n","\n","fig, ax = plt.subplots(figsize=(10, 10), dpi=100)\n","n_cars = -1\n","ax.scatter(Data2d[:n_cars,0], Data2d[:n_cars,1]) \n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","for i,car in enumerate(list_cars[:n_cars]):\n","    plt.annotate(car,(Data2d[i,0], Data2d[i,1]),fontsize=6)\n","\n","    \n","cars = ['ChevroletCorvette','DodgeStealth','GeoMetro','HondaCivic','ChevroletAstro','ToyotaPrevia']\n","cars_index = [list_cars.index(nombre) for nombre in cars]\n","zoom_image = [0.1,0.07,0.2,0.03,0.05,0.3]\n","offset_image = [-0.5,-0.5,-0.5,-0.5,0.5,0.5]\n","\n","for i,car in enumerate(cars):\n","    \n","    fig_name = car + '.jpg'\n","    \n","    image = io.imread(fig_name)\n","\n","    im = OffsetImage(image, zoom=zoom_image[i])\n","    ab = AnnotationBbox(im, Data2d[cars_index[i],:]+offset_image[i], xycoords='data', frameon=False)\n","    ax.add_artist(ab)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5nrdUiIsP7z-"},"source":["En el gráfico anterior, podemos comprobar como de forma no supervisada, en la proyección que PCA encuentra en solo dos dimensiones hay grupos diferenciados que tienen consistencia, tales como caravanas y furgonetas (Chevrolet Astro, Volkswagen Eurovan, Mazda MPV, ...),  coches compactos (Honda Civic, Ford Fiesta, Geo Metro...) o deportivos (Chevrolet Corvette, Mazda RX7). En este sentido, PCA permite una visualización cualitativa de los datos que puede ser útil en muchos problemas."]},{"cell_type":"markdown","metadata":{"id":"IQGyrdiNP70A"},"source":["## ¿Cuántas componentes son suficientes para representar nuestros datos?\n","\n","En el ejemplo anterior hemos tomado $K=2$ para poder visualizar la proyección de PCA. Una opción habitual para escoger el número de componentes es utilizar la fracción de la **varianza explicada** \n","\n","$$F(K) = \\frac{\\sum_{j=1}^{K}\\lambda_j}{\\sum_{j=1}^{K_{max}} \\lambda_j}$$\n","\n","donde el denominador (suma de todos los autovalores de la matriz de covarianza) se denomina **varianza total** y el numerador corresponde la varianza explicada por las $K$ componentes principales. Una opción usual es buscar $K$ de tal manera que $F(K)$ se sitúe en el $90\\%$ o $95\\%$.\n","\n","Vamos a dibujar $F(K)$ para el dataset anterior."]},{"cell_type":"code","metadata":{"id":"QmJFqQoHP70A"},"source":["plt.rcParams.update({'font.size': 12})\n","\n","pca_cars_full = PCA(n_components=Data0.shape[1]) # Dimensión de X menos 1, máximo que podemos escoger\n","\n","pca_cars_full.fit(Data)\n","\n","plt.stem(np.arange(1,Data0.shape[1]+1,1),pca_cars_full.explained_variance_)\n","plt.xlabel('Componente PCA')\n","plt.ylabel('Varianza explicada $\\lambda_j$')\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","# Calculamos F(K)\n","plt.figure()\n","F_K = [np.sum(pca_cars_full.explained_variance_[:l])/np.sum(pca_cars_full.explained_variance_) for l in range(1,Data0.shape[1]+1)]\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","plt.stem(range(1,Data0.shape[1]+1),F_K)\n","plt.ylim([0.6,1.0])\n","plt.xlabel('Número K de Componentes principales')\n","plt.ylabel('F(K)')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X31pAiyDP70F"},"source":["Podemos ver que que con 3 componentes $F(K)$ está ya por encima del $90\\%$. "]},{"cell_type":"markdown","metadata":{"id":"eTyht2R5P70F"},"source":["## Extracción de características PCA\n","\n","Otro de los usos más comunes con PCA es la reducción de dimensionalidad de los datos a la entrada de un regressor o un clasificador. Esto nos puede permitir combatir el sobre ajuste debido a la alta dimensionalidad de los datos. \n","\n","Usando de nuevo la base de datos Spam, vamos a generar un ejemplo con muy pocos datos en el que un clasificador que trabaje sobre los datos originales va a sobreajustar. Utilizando PCA, reduciremos la dimensionalidad de los datos y mejoraremos nuestro clasificador en el conjunto de test."]},{"cell_type":"code","metadata":{"id":"-CDMaduMP70G"},"source":["data = load_spam()\n","X0_spam = data[data.columns[:57]].values\n","Y_spam = data['target'].values\n","\n","print(\"Cargadas {0:d} observaciones con {1:d} columnas\\n\".format(len(data), len(data.columns)))\n","print(\"Ejemplos\")\n","data.head()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJXEDV6dP70R"},"source":["# Dividimos train/test. Normalizamos\n","X0_spam_train, X0_spam_test, Y_spam_train, Y_spam_test = train_test_split(X0_spam, Y_spam, test_size=0.3, random_state=42)\n","\n","transformer_spam = StandardScaler().fit(X0_spam_train)  # X0--> Datos originales, X --> Normalizados\n","X_spam_train = transformer_spam.transform(X0_spam_train)\n","X_spam_test = transformer_spam.transform(X0_spam_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mEb85eFjP70W"},"source":["Vamos a reducir el conjunto de entrenamiento a sólo **100 datos**"]},{"cell_type":"code","metadata":{"id":"nV4y5Ak-P70Z"},"source":["X_spam_train = X_spam_train[:100,:]\n","Y_spam_train = Y_spam_train[:100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XRlblQUiP70d"},"source":["Y ahora entrenamos un k-NN sobre estos 100 datos de dimensión 58 ..."]},{"cell_type":"code","metadata":{"id":"YDBq_14QP70d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668092020623,"user_tz":-60,"elapsed":1358,"user":{"displayName":"AURORA COBO AGUILERA","userId":"02417368943911432830"}},"outputId":"8a9a8c26-0bd8-4919-8f86-e81eb75d9b5f"},"source":["from sklearn import neighbors\n","# Entrenamiento k-NN con validación de vecinos\n","\n","K_max = 15\n","rango_K = np.arange(1, K_max+1)\n","nfold = 10\n","\n","# Define un diccionario con el nombre de los parámetros a explorar como claves y el rango como valores\n","diccionario_parametros = [{'n_neighbors': rango_K,'weights':['uniform','distance']}]\n","\n","# Validación cruzada con GridSearchCV\n","knn_spam = GridSearchCV(estimator=neighbors.KNeighborsClassifier( ), param_grid=diccionario_parametros,cv=nfold)\n","# Entrenamiento\n","knn_spam.fit(X_spam_train,Y_spam_train)\n","# Test\n","accuracy_train_knn = knn_spam.score(X_spam_train,Y_spam_train)\n","accuracy_test_knn = knn_spam.score(X_spam_test,Y_spam_test)\n","\n","print(\"El número de vecinos seleccionado es k={0:d}\".format(knn_spam.best_params_['n_neighbors']))\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train_knn*100, accuracy_test_knn*100))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El número de vecinos seleccionado es k=5\n","Accuracy train 87.00%. Accuracy test 81.10%\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"2hC3gQLSP70i"},"source":["Podemos comprobar cómo en el conjunto de test el rendimiento del clasificador empeora. Para reducir el impacto del sobreajuste, vamos a utilizar PCA para reducir la dimensionalidad de los datos a la entrada del clasificador. Utilizando `Pipeline` y `GridSearchCV` validaremos el número de componentes de PCA."]},{"cell_type":"code","metadata":{"id":"yLEPbMoyP70j"},"source":["from sklearn.pipeline import Pipeline\n","\n","pipe = Pipeline([('PCA', PCA()),\n","                 ('kNN', neighbors.KNeighborsClassifier( ))])\n","\n","param_grid = {\n","    'PCA__n_components': np.arange(1,20,1),\n","    'kNN__n_neighbors': np.arange(1,K_max+1),\n","    'kNN__weights':['uniform','distance'],\n","}\n","\n","grid_knn = GridSearchCV(pipe, param_grid, cv=3)\n","grid_knn.fit(X_spam_train, Y_spam_train)\n","\n","# Test\n","accuracy_train_knn = grid_knn.score(X_spam_train,Y_spam_train)\n","accuracy_test_knn = grid_knn.score(X_spam_test,Y_spam_test)\n","\n","print(\"El número de componentes de PCA es {0:d}\".format(grid_knn.best_estimator_['PCA'].n_components))\n","print(\"El número de vecinos seleccionado es k={0:d}\".format(grid_knn.best_estimator_['kNN'].n_neighbors))\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train_knn*100, accuracy_test_knn*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uopG_n4vP70n"},"source":["Observad que con sólo **4 componentes** hemos conseguido mejorar la fracción de etiquetas correctas en el test set al 86\\%.\n","\n","Es importante notar que la selección del número de componentes no sigue el criterio de conseguir explicar el 90\\% de la varianza total. De hecho si dibujamos $F(K)$ en este caso podemos comprobar que para $K=4$, $F(K)$ está por debajo del 40%. Sin embargo, esta representación de baja dimensión permite generalizar mejor a los datos de test."]},{"cell_type":"code","metadata":{"id":"UiM6iDjnP70o"},"source":["pca_spam_full = PCA(n_components=X_spam_train.shape[1]).fit(X_spam_train)\n","\n","\n","# Calculamos F(K)\n","F_K = [np.sum(pca_spam_full.explained_variance_[:l])/np.sum(pca_spam_full.explained_variance_) for l in range(1,X_spam_train.shape[1]+1)]\n","\n","plt.stem(range(1,X_spam_train.shape[1]+1),F_K)\n","plt.xlabel('Número K de Componentes principales')\n","plt.ylabel('F(K)')\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWcTfX9mP70s"},"source":["> **Ejercicio:**  Determinar el porcentaje de etiquetas correctamente clasificadas en train y test si usamos un número de componentes PCA que explique el 93% de la varianza total."]},{"cell_type":"code","metadata":{"id":"MpmuTpxiP70u"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XgvK2vFcP70x"},"source":["<a id='kmeans'></a>\n","\n","# Parte II: Agrupación de datos (clustering) con k-means\n","---\n","\n","El algoritmo k-means es un algoritmo particularmente sencillo que nos permite encontrar grupos (clusters) sobre un conjunto de datos de forma **no supervisada**. Estos grupos pueden ser útiles para entender la estructura de nuestros datos, imputar valores perdidos en nuestros datos, realizar etiquetado de forma más eficiente, detección de outliers, o para encontrar grupos de datos similares (*information retrieval*).\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d_WVwh2uP70y"},"source":["\n","## Algoritmo K-means\n","---\n","\n","Dado un dataset de $\\{\\mathbf{x}_n\\}_{n=1}^N$, una métrica de **distancia** entre cualquier par de puntos $d(\\mathbf{x},\\mathbf{x}')$ y un número $K$ prefijado de grupos, K-means implementa el siguiente algoritmo iterativo para asignar cada dato a una de los $K$ posibles grupos:\n","\n","**Inicialización**. Cada grupo $k$, $k=1,\\ldots,K$, se caracteriza por un *centroide* que $\\mathbf{c}_k$ que inicializa (en la versión de k-means más sencilla) escogiendo aleatoriamente un punto de nuestro dataset. \n","\n","Hasta **convergencia** se repiten los siguientes pasos:\n","\n","- **Paso I: asignación**. Para $n=1,\\ldots,N$, el dato $\\mathbf{x}_n$ se asigna al grupo cuyo centroide esté más cerca de acuerdo a la métrica de distancia. Esto es, si $A(\\mathbf{x}_n)\\in\\{1,\\ldots,K\\}$ representa el grupo asignado al dato $\\mathbf{x}_n$ entonces\n","\n","\\begin{align}\n","A(\\mathbf{x}_n) = \\arg \\min_{\\{1,\\ldots,K\\}} d(\\mathbf{x}_n,\\mathbf{c}_k)\n","\\end{align}\n","\n","- **Paso II: actualizar centroides**. Dadas las asignaciones de los datos a los grupos, recalculamos el centroide como la media aritmética de los puntos asignados a  cada una de ellas. Para $k=1,\\ldots,K$,\n","\n","\\begin{align}\n","\\mathbf{c}_k = \\frac{1}{N_k} \\sum_{n: A(\\mathbf{x}_n)= k} \\mathbf{x}^{(n)},\n","\\end{align}\n","$~~~~~~$ donde $\\frac{1}{N_k}$ es el número de puntos asignados al grupo $k$.\n","\n","Si no hay cambios en la asignación de puntos a grupos entre dos iteraciones consecutivas, el algoritmo ha convergido. En caso contrario, volvemos al paso 2.\n","\n","\n","<img src='http://www.tsc.uc3m.es/~olmos/BBVA/k-mean_good.jpg' width=1000 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ifK_ZDBgP70y"},"source":["## Un ejemplo sintético en dos dimensiones\n","---\n","\n","Como es habitual, vamos a mostrar el uso de la librería [KMeans de Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) y su solución con un ejemplo sintético en dos dimensiones.\n","\n"]},{"cell_type":"code","metadata":{"id":"mnbydS-pP70y","tags":[]},"source":["from sklearn.datasets import make_blobs\n","\n","rng = np.random.RandomState(36)\n","\n","X, y_true = make_blobs(n_samples=500, centers=6,\n","                       cluster_std=0.6, random_state=22)\n","\n","plt.scatter(X[:, 0], X[:, 1], s=50)\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('$x_1$')\n","plt.ylabel('$x_2$')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCxfOPWBP703"},"source":["from sklearn.cluster import KMeans\n","\n","# K-means con K=4. Luego hablamos sobre la selección de K\n","K=4\n","\n","kmeans = KMeans(n_clusters=K) # Definimos objeto con parámetros por defecto\n","kmeans.fit(X) # Entrenamos k-means\n","y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","centers = kmeans.cluster_centers_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DdLCOT0ajivp"},"source":["y_kmeans"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TjkX1yuDP706"},"source":["plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","\n","plt.scatter(centers[:, 0], centers[:, 1], c='black', s=500, alpha=0.5)\n","\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","plt.xlabel('$x_1$')\n","plt.ylabel('$x_2$')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fHqsHKOIP70-"},"source":["## Sensibilidad con la inicialización\n","---\n","\n","En la práctica, el algoritmo k-means es muy rápido (uno de los algoritmos de agrupamiento más rápidos disponibles), pero su solución no es única (hay muchos mínimos locales). Por eso puede ser útil reiniciarlo varias veces.\n","\n","Vamos a comprobar en el ejemplo anterior las soluciones con distintas inicializaciones. En las siguientes figuras, mostramos la **inicialización de cada centroide con un triángulo rojo**.\n"]},{"cell_type":"code","metadata":{"id":"k40sjgWcP70_"},"source":["n_plots_row = 3\n","n_rows = 2\n","rs = 45\n","\n","fig, ax = plt.subplots(nrows=n_rows, ncols=n_plots_row, figsize=(16, 13))\n","\n","for r in range(n_rows):\n","    for c in range(n_plots_row):\n","        \n","        # Inicialización aleatoria en el dataset\n","        centers_idx = np.random.permutation(X.shape[0])[:4]\n","        init_centroids = X[centers_idx,:]\n","        kmeans = KMeans(n_clusters=K,init=init_centroids,random_state=rs) \n","        kmeans.fit(X) # Entrenamos k-means\n","        y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","        \n","        centers = kmeans.cluster_centers_\n","        \n","        ax[r,c].scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","        ax[r,c].scatter(init_centroids[:,0], init_centroids[:, 1], c='red', s=500, alpha=0.5,\n","                        marker=\"v\",label='Inicialización')\n","        ax[r,c].scatter(centers[:, 0], centers[:, 1], c='black', s=500, alpha=0.5,label='Centroide k-means')\n","        ax[r,c].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","        ax[r,c].set_xlabel('$x_1$')\n","        ax[r,c].set_ylabel('$x_2$')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joqoA7aDP71B"},"source":["¿Cómo escoger qué solución de K-means es la más adecuada? Podemos nombrar dos opciones básicas.\n","\n","1. Si buscamos grupos **interpretables** (por ejemplo agrupar clientes de un determinado servicio bancario), tendremos que mirar con atención los grupos y decidir qué solución de K-means se ajusta de forma más coherente a  los datos y el problema en cuestión. Típicamente, esto lo haría un experto en el problema en sí (no tanto una persona que únicamente sepa de aprendizaje automático).\n","\n","\n","2. Podemos utilizar la **función de coste de K-means** u otros coeficientes que midan la **consistencia de los grupos encontrados**.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HVv89MIiP71C"},"source":["## Función de coste en K-means\n","---\n","\n","El algoritmo iterativo de $K$-means que hemos presentado anteriormente es una forma de encontrar la solución al siguiente problema\n","\n","\\begin{align}\n","\\min_{\\mathbf{c}_1,\\ldots,\\mathbf{c}_K} \\mathcal{L}(\\mathbf{c}_1,\\ldots,\\mathbf{c}_K) = \\min_{\\mathbf{c}_1,\\ldots,\\mathbf{c}_K} \\frac{1}{N} \\sum_{k=1}^K \\sum_{i: A(\\mathbf{x}^{(i)})= k} d(\\mathbf{x}^{(i)},\\mathbf{c}_k)\n","\\end{align}\n","\n","donde fijaos que la función de coste $\\mathcal{L}(\\mathbf{c}_1,\\ldots,\\mathbf{c}_K)$ es la **distancia promedio de cada punto a su correspondiente centroide**. En K-means, pretendemos minimizar esta cantidad. Sin embargo, como hemos comentado esta función de coste tiene en general varios mínimos locales.\n","\n","Una opción para escoger la solución de K-means es buscar aquella que minimiza $\\mathcal{L}(\\mathbf{c}_1,\\ldots,\\mathbf{c}_K)$ tras repetir el algoritmo un cierto número de veces. En [K-means con sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) accedemos al valor de $\\mathcal{L}(\\mathbf{c}_1,\\ldots,\\mathbf{c}_K)$ con el atributo `inertia_` si lo dividimos por el número de datos.\n","\n","\n","Estudiemos el valor de $L$ en el caso anterior."]},{"cell_type":"code","metadata":{"id":"JFX0Cx12P71C"},"source":["n_plots_row = 3\n","n_rows = 2\n","rs = 45\n","\n","fig, ax = plt.subplots(nrows=n_rows, ncols=n_plots_row, figsize=(16, 13))\n","\n","for r in range(n_rows):\n","    for c in range(n_plots_row):\n","        \n","        # Inicialización aleatoria en el dataset\n","        centers_idx = np.random.permutation(X.shape[0])[:4]\n","        init_centroids = X[centers_idx,:]\n","        kmeans = KMeans(n_clusters=K,init=init_centroids,random_state=rs) \n","        kmeans.fit(X) # Entrenamos k-means\n","        y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","        \n","        centers = kmeans.cluster_centers_\n","        \n","        ax[r,c].scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","        ax[r,c].scatter(init_centroids[:,0], init_centroids[:, 1], c='red', s=500, alpha=0.5,\n","                        marker=\"v\",label='Inicialización')\n","        ax[r,c].scatter(centers[:, 0], centers[:, 1], c='black', s=500, alpha=0.5,label='Centroide k-means')\n","        ax[r,c].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","        ax[r,c].text(-11, 8, 'L={0:.2f}'.format(kmeans.inertia_/X.shape[0]), fontsize=13)\n","        ax[r,c].set_xlabel('$x_1$')\n","        ax[r,c].set_ylabel('$x_2$')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0yGdfmIMP71F"},"source":["Esta selección de la mejor solución de K-means la podemos implementar directamente en Sklearn con el **parámetro de entrada** `n_init`"]},{"cell_type":"code","metadata":{"id":"l56Kiu02P71F"},"source":["# K-means con K=4. Luego hablamos sobre la selección de K\n","K=4\n","\n","kmeans = KMeans(n_clusters=K,n_init=10) # Definimos objeto con parámetros por defecto\n","kmeans.fit(X) # Entrenamos k-means\n","y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","centers = kmeans.cluster_centers_\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","\n","plt.scatter(centers[:, 0], centers[:, 1], c='black', s=500, alpha=0.5)\n","\n","plt.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","plt.xlabel('$x_1$')\n","plt.ylabel('$x_2$')\n","plt.text(-11, 8, 'L={0:.2f}'.format(kmeans.inertia_/X.shape[0]), fontsize=13)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sa3gbhJNP71L"},"source":["## Selección del número $K$ de grupos\n","---\n","\n","La elección del número $K$ de grupos puede hacerse con consideraciones similares. \n","\n","- Si buscamos interpretabilidad de los grupos, exploraremos la solución para $K=1,2,3, ...$ hasta encontrar qué caso separa los datos de forma interpretable para un experto.\n","\n","- De nuevo, podemos usar la función de coste $\\mathcal{L}(\\mathbf{c}_1,\\ldots,\\mathbf{c}_K)$, pero aquí debemos de tener cuidado. Vamos a representar esta función para distintos valores de $K$ en el caso anterior. "]},{"cell_type":"code","metadata":{"id":"g7n0GlShP71L"},"source":["K_list = range(2,20)\n","\n","L = []\n","\n","for k in K_list:\n","    \n","    kmeans = KMeans(n_clusters=k,n_init = 10) # Para cada valor de $K$ lanzamos 10 veces K-means\n","    kmeans.fit(X)\n","    L.append((kmeans.inertia_)/(X.shape[0]))\n","    \n","# Solución para el último valor de K\n","\n","y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","centers = kmeans.cluster_centers_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZ5-mDKGP71O"},"source":["fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n","\n","ax[0].plot(6,L[5],'rs',ms=15,label='Codo')\n","ax[0].plot(K_list,L,'-o')\n","ax[0].legend()\n","\n","ax[0].set_xlabel('$K$')\n","ax[0].set_ylabel('$L(K)$')\n","ax[0].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax[0].set_xticks(K_list);\n","\n","ax[1].scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n","\n","ax[1].scatter(centers[:, 0], centers[:, 1], c='black', s=500, alpha=0.5)\n","\n","ax[1].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","ax[1].set_xlabel('$x_1$')\n","ax[1].set_ylabel('$x_2$')\n","ax[1].text(-11, 8, 'L={0:.2f}'.format(kmeans.inertia_/X.shape[0]), fontsize=13)\n","ax[1].set_title('K-means para K={0:d}'.format(K_list[-1]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BPOdQL46P71S"},"source":["Fijaos que $L(K)$ es una función monotona decreciente con $K$. De hecho, para $K=N$ tendremos un grupo por punto y por tanto haremos $L(K=N)=0$. Esto es un signo claro de **sobreajuste**. En general, dado $L(K)$ buscaremos aquel/aquellos valores de $K$ a partir de los cuales $L(K)$ desciende más lentamente, esto se denomina el **codo de la curva**. En el caso anterior coincide con $K=6$ el número de grupos que habíamos especificado al generar el dataset."]},{"cell_type":"markdown","metadata":{"id":"sAr1vk0eP71T"},"source":["##  Coeficiente de silueta (silhouette score)\n","---\n","\n","De forma alternativa a estudiar $L(K)$ para la selección del número $K$ de grupos, podemos utilizar el denominado coeficiente de silueta ([silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))). Este valor es una medida de cuán similar es un objeto a su propio cúmulo (cohesión) en comparación con otros cúmulos (separación).\n","\n","Para cada punto $\\mathbf{x}_n$, este coeficiente se calcula del siguiente modo\n","\n","$$ s(\\mathbf{x}_n) = \\frac{b(\\mathbf{x}_n)-a(\\mathbf{x}_n)}{\\max\\Big\\{b(\\mathbf{x}_n),a(\\mathbf{x}_n)\\Big\\}}$$\n","\n","donde \n","\n","- para calcular $b(\\mathbf{x}_n)$, calculamos el promedio de distancia de $\\mathbf{x}_n$ a todos los puntos de cada grupo al que $\\mathbf{x}_n$ **no pertenece**. $b(\\mathbf{x}_n)$ es el mínimo entre esta distancia promedio por grupo. Por tanto, si $\\mathbf{x}_n$ está en el grupo $i$\n","\n","$$b(\\mathbf{x}_n) = \\min_{k\\neq i} \\frac{1}{N_k} \\sum_{j\\in\\mathcal{C}_k} d(\\mathbf{x}_n,\\mathbf{x}_j)$$\n","\n","- $a(\\mathbf{x}_n)$ es la distancia promedio de $\\mathbf{x}_n$ a todos los puntos de su grupo\n","\n","$$a(\\mathbf{x}_n) = \\frac{1}{N_i-1} \\sum_{j\\in\\mathcal{C}_i,i\\neq n} d(\\mathbf{x}_n,\\mathbf{x}_j)$$\n","\n","\n","El valor del coeficiente de silueta está entre [-1, 1]. Una puntuación de 1 implica que el dato está cohesionado dentro del grupo al que pertenece y está lejos de los otros grupos. El peor valor es -1. Los valores cercanos a 0 denotan grupos superpuestos.\n","\n","Con [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) podemos obtener el coeficiente promedio para todos los puntos.\n","\n","Dibujemos el coeficiente silueta para distintos valores de $K$."]},{"cell_type":"code","metadata":{"id":"5AVMhohYP71T"},"source":["from sklearn.metrics import silhouette_score\n","\n","K_list = range(2,20)\n","\n","L = []\n","SC = []\n","\n","for k in K_list:\n","    \n","    kmeans = KMeans(n_clusters=k,n_init = 10) # Para cada valor de $K$ lanzamos 10 veces K-means\n","    kmeans.fit(X)\n","    L.append((kmeans.inertia_)/(X.shape[0]))\n","    y_kmeans = kmeans.predict(X) # Obtenemos el identificador del grupo para cada dato\n","    SC.append(silhouette_score(X,y_kmeans))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etQs2V86P71V"},"source":["fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n","\n","ax[0].plot(6,L[5],'rs',ms=15,label='Codo')\n","ax[0].plot(K_list,L,'-o')\n","ax[0].legend()\n","ax[0].set_xticks(K_list);\n","\n","ax[0].set_xlabel('$K$')\n","ax[0].set_ylabel('$L(K)$')\n","ax[0].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","#ax[1].plot(6,L[5],'rs',ms=15,label='Codo')\n","ax[1].plot(K_list[np.argmax(SC)],np.max(SC),'rs',ms=15,label='Maximo')\n","ax[1].plot(K_list,SC,'-o')\n","ax[1].legend()\n","\n","ax[1].set_xlabel('$K$')\n","ax[1].set_ylabel('Coeficiente Silueta Promedio')\n","ax[1].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax[1].set_xticks(K_list);\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSFhWmPkP71Y"},"source":["Podemos observar que la determinación de $K=6$ en base a coeficiente de silueta parece más clara."]},{"cell_type":"markdown","metadata":{"id":"sW1u6oYOP71Y"},"source":["## K-means sobre la base de datos Wine\n","---\n","\n","A continuación vamos a explorar la base de datos [Wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine), que son el resultado de un análisis químico de vinos cultivados en la misma región en Italia pero de **tres variedades diferentes**. El análisis determinó las cantidades de 13 componentes encontrados en cada uno de los tres tipos de vinos.\n","\n","Esta es una base de datos etiquetada (sabemos qué tipo de vino es cada dato). A modo de ejemplo, estudiaremos si K-means es capaz de separar las categorias de forma no supervisada.\n"]},{"cell_type":"code","metadata":{"id":"zk5fhOueP71Z"},"source":["data = load_wine()\n","\n","data.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjKRv2OuP71b"},"source":["> **Ejercicio:** Excluyendo la columna `Wine_type`, estandarice la base de datos en media y varianza. A continuación, realice un agrupamiento de datos con K-means, usando el coeficiente de silueta para determinar el número adecuado de grupos."]},{"cell_type":"code","metadata":{"id":"C-_e6UpjP71c"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7rWsTYfP71e"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RjP5cQTbP71h"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ByU0Lx6P71j"},"source":["> **Ejercicio:** Para el número $K$ de grupos seleccionado, estudie cómo las etiquetas verdaderas se distribuyen a lo largo de los grupos encontrados."]},{"cell_type":"code","metadata":{"id":"C0PRFTWAP71k"},"source":["#<SOL>\n","\n","\n","\n","\n","\n","y_kmeans =\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OX4_L9sP71m"},"source":["# Dibujemos el histograma de etiquetas REALES presentes en cada grupo\n","\n","fig, ax = plt.subplots(nrows=1, ncols=K_final , figsize=(12, 6))\n","\n","d = np.diff(np.unique(wine_types)).min()\n","left_of_first_bin = wine_types.min() - float(d)/2\n","right_of_last_bin = wine_types.max() + float(d)/2\n","\n","for i in range(K_final):\n","    \n","    ax[i].hist(wine_types[y_kmeans==i], np.arange(left_of_first_bin, right_of_last_bin + d, d))\n","    ax[i].grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","    ax[i].set_title(\"Etiquetas grupo {0:d}\".format(i))\n","    ax[i].set_xticks(np.arange(0,4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57FAzFgHP71p"},"source":["Podemos comprobar cómo hay una correspondencia casi unívoca entre etiquetas y grupos."]},{"cell_type":"markdown","metadata":{"id":"jKGPDqsoP71q"},"source":["<a id='GMM'></a>\n","\n","# Parte III: Estimación de la densidad de probabilidad con modelos de mezcla de Gaussianas\n","\n","Los [modelos de mezcla](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch20.pdf) son un tipo de modelo probabilístico de datos que permiten tanto aproximar la **densidad de probabilidad de los datos** como hacer **agrupamientos** de los mismos. \n","\n","En esta sesión, nos centramos en modelos de mezcla basados en distribuciones Gaussianas o [**Gaussian Mixture Models** (GMM)](https://brilliant.org/wiki/gaussian-mixture-model/). \n"]},{"cell_type":"markdown","metadata":{"id":"cO6fuISlP71q"},"source":["## Limitaciones de K-means\n","\n","Usando GMMs, podemos solventar dos **limitaciones importantes** de K-means:\n","\n","- K-means asume de forma implícita que los **grupos son circulares** (todos los puntos que están a la misma del centroide).\n","\n","- K-means hace una asignación **dura** de cada punto a un grupo. No proporciona una medida de incertidumbre para identificar puntos que estan cerca de varios grupos.\n","\n","Vamos a ilustrar estas limitaciones en un dataset sintético. Ejemplos tomados de esta [referencia](https://www.oreilly.com/library/view/python-data-science/9781491912126/).\n"]},{"cell_type":"code","metadata":{"id":"xlvylJOiP71r"},"source":["from scipy.spatial.distance import cdist\n","\n","X, y_true = make_blobs(n_samples=400, centers=4,\n","                       cluster_std=0.60, random_state=0)\n","\n","kmeans = KMeans(4, random_state=0)\n","labels = kmeans.fit(X).predict(X)\n","centers = kmeans.cluster_centers_\n","\n","fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.axis('equal')\n","ax.scatter(centers[:, 0], centers[:, 1], c='black', s=50, alpha=0.5)\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","\n","for i,c in enumerate(centers):\n","    radio = np.linalg.norm(X[labels == i]-c,axis=1).max()\n","    ax.add_patch(plt.Circle(c,radio,alpha=0.2,lw=3))\n","\n","ax.text(7, 5, 'A pesar de la cercanía de los grupos,  K-means da una \\núnica asignación por dato', fontsize=15);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2hJ7h5sP71t"},"source":["rng = np.random.RandomState(13)\n","X_stretched = np.dot(X, rng.randn(2, 2))\n","\n","kmeans = KMeans(4, n_init=10)\n","labels = kmeans.fit(X_stretched).predict(X_stretched)\n","centers = kmeans.cluster_centers_\n","\n","fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.scatter(X_stretched[:, 0], X_stretched[:, 1], c=labels, s=40, cmap='viridis')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.axis('equal')\n","ax.scatter(centers[:, 0], centers[:, 1], c='black', s=50, alpha=0.5)\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","\n","for i,c in enumerate(centers):\n","    radio = np.linalg.norm(X_stretched[labels == i]-c,axis=1).max()\n","    ax.add_patch(plt.Circle(c,radio,alpha=0.2,lw=3))\n","\n","ax.text(3.5, 2, 'Regiones circulares no parecen ser las adecuadas para \\nadaptarse a estos datos', fontsize=15);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvpWRITCP71v"},"source":["##  Modelos de mezcla de Gaussianas\n","\n","En un GMM, ajustaremos a nuestros datos una función densidad de probabilidad parametrizada del siguiente modo:\n","\n","\\begin{align}\n","p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_k,\\mathbf{\\Sigma}_k)\n","\\end{align}\n","\n","donde\n","\n","- $\\pi_k$ es la probabilidad de que el dato provenga del grupo $k$-ésimo.\n","\n","\n","- Asumimos que todos los datos asignados al grupo $k$-ésimo se distribuyen según una Gaussiana de media $\\mathbf{\\mu}_k$ y matriz de covarianza $\\mathbf{\\Sigma}_k$. \n","\n","\n","- $(\\pi_1,\\ldots,\\pi_K)$, $(\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_K)$, $(\\mathbf{\\Sigma}_1,\\ldots,\\mathbf{\\Sigma}_K)$ son los **parámetros del modelo**.\n","\n","## Entrenamiento de un GMM\n","\n","Los parámetros del modelo se escogen para maximizar la **probabilidad de los datos ya observados o evidencia**:\n","\n","$$ \\max_{(\\pi_1,\\ldots,\\pi_K),(\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_K), (\\mathbf{\\Sigma}_1,\\ldots,\\mathbf{\\Sigma}_K)}  ~~\\sum_{n=1}^{N} \\log  p(\\mathbf{x}_n) $$\n","\n","Este problema se resuelve de forma numérica mediante un algoritmo iterativo conocido como **EM** (Expectation-Maximization).\n","\n","Vamos a entrenar un GMM usando [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) sobre el último ejemplo con el que hemos trabajado."]},{"cell_type":"code","metadata":{"id":"gJbM57rfP71v"},"source":["fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.scatter(X_stretched[:, 0], X_stretched[:, 1], s=40, cmap='viridis')\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"abUxBM-4P710"},"source":["from sklearn.mixture import GaussianMixture\n","\n","# Seguimos teniendo mínimos locales. Relanzamos 10 veces y nos quedamos con el mejor\n","# Full para matrices de covarianza completas\n","\n","gmm = GaussianMixture(n_components=K,covariance_type='full',n_init=10) \n","\n","gmm.fit(X_stretched) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"34ycTwDoP713"},"source":["Dibujemos las medias $(\\mathbf{\\mu}_1,\\ldots,\\mathbf{\\mu}_K)$ sobre los datos. Estos valores están accesibles a través de `gmm.means_`."]},{"cell_type":"code","metadata":{"id":"vvTLitRPP713"},"source":["fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.scatter(X_stretched[:, 0], X_stretched[:, 1], s=40, cmap='viridis')\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","\n","ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='black', s=200, alpha=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ffBf24WP717"},"source":["Y ahora las curvas de contorno, esto es, las curvas sobre el espacio $(x_1,x_2)$ con igual probabilidad $p(\\mathbf{x})$. También la propia $p(\\mathbf{x})$ en una figura en 3D."]},{"cell_type":"code","metadata":{"id":"69688l-ZP717"},"source":["from scipy.stats import multivariate_normal as mvn # Distribución normal/Gaussiana multivaraible\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","\n","# Dibujemos el contorno de la función de densidad\n","intervals = 200\n","\n","# Creamos una rejilla\n","x = np.linspace(-3, 3, intervals)\n","y = np.linspace(-1.5, 4.5, intervals)\n","\n","X,Y = np.meshgrid(x,y)\n","\n","xys = np.vstack([X.ravel(), Y.ravel()]).T\n","\n","# Evaluamos p(x) para cada punto de la rejilla\n","\n","K=4\n","\n","Zgmm = np.zeros(len(xys))\n","for k in range(K):\n","    Zgmm += gmm.weights_[k]*mvn(gmm.means_[k,:], gmm.covariances_[k]).pdf(xys)\n","    \n","fig = plt.figure(figsize=(14, 7))\n","ax = fig.add_subplot(1, 2, 1)\n","\n","Zgmm = Zgmm.reshape([intervals,intervals])\n","ax.contour(X, Y, Zgmm, 20, cmap=cm.coolwarm) \n","ax.scatter(X_stretched[:, 0], X_stretched[:, 1], s=40, cmap='viridis')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$')\n","\n","ax = fig.add_subplot(1, 2, 2, projection='3d')\n","surf = ax.plot_surface(X, Y, Zgmm, cmap=cm.coolwarm)\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","fig.colorbar(surf, shrink=0.5, aspect=5)\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_fhYu7BwP719"},"source":["## Agrupando datos usando un GMM\n","\n","Una vez ajustado el GMM, dado un dato $\\bf x$, podemos calcular la **probabilidad a posteriori** del grupo a partir de la cual se generó dicho dato:\n","\n","\n","$$P(\\text{cluster}=k|\\mathbf{x})= \\frac{\\pi_{k} \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_{k},\\mathbf{\\Sigma}_{k})}{\\sum_{q=1}^K \\pi_{q} \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu}_{q},\\mathbf{\\Sigma}_{q})}$$\n","\n","Esto significa que tenemos una **probabilidad de pertenecer a cada grupo**, en lugar de una asignación dura que es lo que proporcionaba K-means.\n","\n","En Sklearn, podemos calcular dichas probabilidades usando el método `predict_proba`. Analicemos estas probabilidades para los 4 puntos marcados con cruces en la siguiente figura."]},{"cell_type":"code","metadata":{"id":"0PXcKZWSP71-"},"source":["puntos = [[0,1.5],[-1,2],[2,1],[2,4]]\n","\n","fig,ax = plt.subplots(figsize=(7, 7))\n","\n","Zgmm = Zgmm.reshape([intervals,intervals])\n","ax.contour(X, Y, Zgmm, 20, cmap=cm.coolwarm) \n","ax.scatter(X_stretched[:, 0], X_stretched[:, 1], s=40, cmap='viridis')\n","for p in puntos:\n","    ax.plot(p[0],p[1],'gx',ms=15,markeredgewidth=4)\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.set_xlabel('$x_1$')\n","ax.set_ylabel('$x_2$');\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"APFIUg3fP72C","jupyter":{"source_hidden":true},"tags":[]},"source":["np.set_printoptions(suppress=True)\n","np.set_printoptions(precision=2)\n","\n","for p in puntos:\n","    \n","    print(\"Las probabilidades del punto ({0:.1f},{1:.1f}) son\".format(p[0],p[1]))\n","    print(gmm.predict_proba(np.array(p).reshape([1,-1]))) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7bqummWP72G","tags":[]},"source":["Por otra parte, el método `gmm.predict(X)` devuelve el índice del grupo más probable."]},{"cell_type":"markdown","metadata":{"id":"Mmiro5pfP72G"},"source":["## Selección del número $K$ de grupo en la mezcla\n","\n","De forma similar a lo que ocurre en K-means, a medida que aumentamos el número de grupos en un GMM la probabilidad de los datos en nuestro modelo aumenta. "]},{"cell_type":"code","metadata":{"id":"wnlQZkNwP72H"},"source":["K_list = range(1,20)\n","\n","P = []\n","\n","for k in K_list:\n","    \n","    gmm = GaussianMixture(n_components=k,covariance_type='full',n_init=10) \n","\n","    gmm.fit(X_stretched) \n","    \n","    P.append(gmm.score(X_stretched))\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLhYgxzFP72L"},"source":["fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.plot(4,P[3],'rs',ms=15,label='Codo')\n","ax.plot(K_list,P,'-o')\n","ax.legend()\n","ax.set_xlabel('$K$')\n","ax.set_ylabel('Log-probabilidad promedio de los datos')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.set_xticks(K_list);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DS27ReRSP72N"},"source":["En la figura anterior podemos comprobar cómo la probabilidad de los datos aumenta más lentamente a partir de $K=4$, este comportamiento tipo codo nos sugiere que este valor es razonable y que a partir del mismo tenderemos a sobreajustar.\n","\n","Existen varios criterios que nos permiten evaluar la probabilidad de los datos **con una penalización asociada a la complejidad del modelo** (número de grupos y por tanto número de parámetros que se han ajustado). Entre ellas, la **métrica BIC** (Bayesian Information Criterion) es usada habitualmente. A menor BIC, entendemos que el modelo tiene un mejor **compromiso entre explicabilidad y complejidad**.\n","\n","En Sklearn, podemos obtener la métrica BIC con el método `gmm.bic(X)`."]},{"cell_type":"code","metadata":{"id":"o6z5VFXbP72N"},"source":["K_list = range(1,20)\n","\n","BIC = []\n","\n","for k in K_list:\n","    \n","    gmm = GaussianMixture(n_components=k,covariance_type='full',n_init=10) \n","\n","    gmm.fit(X_stretched) \n","    \n","    BIC.append(gmm.bic(X_stretched))\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FgiztONP72P"},"source":["fig, ax = plt.subplots(figsize=(7, 7))\n","\n","ax.plot(K_list[np.argmin(BIC)],np.min(BIC),'rs',ms=15,label='Mínimo')\n","ax.plot(K_list,BIC,'-o')\n","ax.set_xlabel('$K$')\n","ax.set_ylabel('BIC score')\n","ax.grid(b=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.set_xticks(K_list)\n","\n","ax.text(21, 2000, 'La elección $K=4$ es evidente a la vista de la métrica BIC', fontsize=15);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lsJY6EwtP72S"},"source":["> **Ejercicio**: Utilizando el método `gmm.predict`, calcule el coeficiente de silueta para distintos valores de $K$ y dibuje la solución. ¿Es el valor $K=4$ también el más adecuado de acuerdo a esta métrica?"]},{"cell_type":"code","metadata":{"id":"REikSzY3P72T"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RX-n4WiiP72U"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFEFIva4P72h"},"source":["Al basarse en distancias euclídeas, esta métrica funciona mejor para grupos de tipo circular. En este caso, el análisis de esta métrica no coincide con el BIC del GMM."]},{"cell_type":"markdown","metadata":{"id":"KGGvrGMEP72h"},"source":["> **Ejercicio:** Utilice un GMM para explicar los datos del dataset Wine. Represente  en función del número K de grupos la métrica BIC  y el coeficiente de silueta usando el grupo más probable."]},{"cell_type":"code","metadata":{"id":"B4vW-4qnP72h"},"source":["#<SOL>\n","\n","#</SOL> "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2y5l3krsP72j"},"source":["#<SOL>\n","\n","#</SOL>"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SrZ5ykbdP72l"},"source":["Podemos observar cómo el coeficiente de silueta indica el número correcto de etiquetas. Si bien la diferencia con $K=2$ y $K=3$ es reducida."]}]}